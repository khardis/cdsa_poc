{"cells":[{"cell_type":"code","source":["# Set environment and corresponding meta_db\n","environment = \"dev\"  # Change this to \"test\" or \"prod\" as needed\n","\n","if environment == \"test\":\n","    meta_db = \"meta_test_db\"\n","elif environment == \"dev\":\n","    meta_db = \"meta_db\"\n","else:\n","    raise ValueError(f\"Unknown environment: {environment}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"6e9f8c8d-eb02-4b70-a601-54973ff66a9b","normalized_state":"finished","queued_time":"2025-10-23T20:58:49.3972218Z","session_start_time":null,"execution_start_time":"2025-10-23T20:58:49.3985329Z","execution_finish_time":"2025-10-23T20:58:49.7323666Z","parent_msg_id":"9c8e1f7f-5c41-41b6-8e6f-7ed4f22da56f"},"text/plain":"StatementMeta(, 6e9f8c8d-eb02-4b70-a601-54973ff66a9b, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99d635f1-faaf-4c4c-ba1a-6bc6b62239f9"},{"cell_type":"markdown","source":["#### Create meta tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ec7d2b6-05de-4956-b33c-fae1da2ff6f6"},{"cell_type":"code","source":["#---------------------------------------------------------------------------------------------\n","# Name: create_meta_tables.ipynb\n","#---------------------------------------------------------------------------------------------\n","# Purpose: Creates the meta delta tables in lk_cdsa_bronze.meta_db\n","#---------------------------------------------------------------------------------------------\n","# ver.  | date     | author         | change\n","#---------------------------------------------------------------------------------------------\n","# v1    | 09/02/25 | K. Hardis      | Initial Version.\n","#---------------------------------------------------------------------------------------------\n","\n","print('||-------------create_meta_tables.ipynb--------------||')\n","print('||-----')\n","\n","print('||-----')\n","\n","print(f\"||    Environment: {environment}\")\n","print(f\"||    Meta Schema: {meta_db}\")\n","print('||-----')\n","\n","# Set current database context\n","spark.catalog.setCurrentDatabase(meta_db)\n","\n","try:\n","\n","    print('|| 0. Create meta tables if not exists')\n","    print('||-----')\n","\n","    # Create meta tables if not exists\n","    spark.sql(f\"\"\"\n","        -- FILE_STATUS_LKP\n","        CREATE TABLE IF NOT EXISTS {meta_db}.FILE_STATUS_LKP (\n","          file_status STRING NOT NULL,\n","          file_status_id BIGINT,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- PROVIDER\n","        CREATE TABLE IF NOT EXISTS {meta_db}.PROVIDER (\n","          provider_id BIGINT,\n","          provider_name STRING,\n","          provider_code STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- RECEIVER\n","        CREATE TABLE IF NOT EXISTS {meta_db}.RECEIVER (\n","          receiver_id BIGINT,\n","          receiver_name STRING,\n","          receiver_code STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- BATCH\n","        CREATE TABLE IF NOT EXISTS meta_db.BATCH (\n","          batch_id BIGINT,\n","          batch_name STRING,\n","          batch_description STRING,\n","          batch_type STRING,\n","          batch_rows BIGINT,\n","          batch_status STRING,\n","          status_message STRING,\n","          start_time TIMESTAMP,\n","          end_time TIMESTAMP,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- DATA_OBJECT\n","        CREATE TABLE IF NOT EXISTS {meta_db}.DATA_OBJECT (\n","          object_id BIGINT,\n","          object_name STRING,\n","          object_type STRING,\n","          object_description STRING,\n","          database_name STRING,\n","          schema_name STRING,\n","          filename_pattern STRING,\n","          filename_extension STRING,\n","          control_file_flag STRING,\n","          control_file_pattern STRING,\n","          control_file_extension STRING,\n","          encryption_flag STRING,\n","          encryption_type STRING,\n","          compression_flag STRING,\n","          compression_type STRING,\n","          file_encoding STRING,\n","          column_delimiter STRING,\n","          row_delimiter STRING,\n","          field_count BIGINT,\n","          header_row_count BIGINT,\n","          footer_row_count BIGINT,\n","          record_length BIGINT,\n","          file_format STRING,\n","          text_qualifier STRING,\n","          error_row_threshold_pct BIGINT,\n","          required_ind STRING,\n","          file_transfer_method STRING,\n","          landing_directory STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- DATA_FILE\n","        CREATE TABLE IF NOT EXISTS {meta_db}.DATA_FILE (\n","          file_id BIGINT,\n","          object_id BIGINT,\n","          file_pattern STRING,\n","          file_date TIMESTAMP,\n","          file_received_date TIMESTAMP,\n","          file_status STRING,\n","          file_byte_size BIGINT,\n","          filename STRING,\n","          expected_row_count BIGINT,\n","          row_count BIGINT,\n","          fm_file_id BIGINT,\n","          fm_good_record_count BIGINT,\n","          fm_error_record_count BIGINT,\n","          stg_good_record_count BIGINT,\n","          file_path STRING,\n","          batch_id BIGINT,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- EXTRACT_CONFIG\n","        CREATE TABLE IF NOT EXISTS {meta_db}.EXTRACT_CONFIG (\n","          extract_id BIGINT,\n","          extract_name STRING,\n","          extract_type STRING,\n","          target_schema STRING,\n","          target_object STRING,\n","          location STRING,\n","          file_name STRING,\n","          file_name_has_datetime_ind STRING,\n","          format_type STRING,\n","          compression_str STRING,\n","          record_delimiter STRING,\n","          field_delimiter STRING,\n","          file_extension STRING,\n","          escape_character STRING,\n","          escape_unenclosed_field STRING,\n","          date_format STRING,\n","          time_format STRING,\n","          timestamp_format STRING,\n","          binary_format STRING,\n","          field_optionally_enclosed_by STRING,\n","          null_if STRING,\n","          overwrite STRING,\n","          single STRING,\n","          max_file_size BIGINT,\n","          object_schema STRING,\n","          object_name STRING,\n","          sqlquery STRING,\n","          header STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- PROCESS\n","        CREATE TABLE IF NOT EXISTS {meta_db}.PROCESS (\n","          process_id BIGINT,\n","          parent_process_id BIGINT,\n","          process_name STRING,\n","          process_folder STRING,\n","          process_qualifier STRING,\n","          process_type STRING,\n","          process_group STRING,\n","          process_text STRING,\n","          process_path STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- PROCESS_LOG\n","        CREATE TABLE IF NOT EXISTS {meta_db}.PROCESS_LOG (\n","          process_log_id BIGINT,\n","          process_id BIGINT,\n","          source_id BIGINT,\n","          target_id BIGINT,\n","          src_file_id BIGINT,\n","          tgt_file_id BIGINT,\n","          process_execute_id BIGINT,\n","          start_time TIMESTAMP,\n","          end_time TIMESTAMP,\n","          inserted_rows BIGINT,\n","          updated_rows BIGINT,\n","          deleted_rows BIGINT,\n","          error_row_count BIGINT,\n","          status STRING,\n","          status_message STRING,\n","          batch_id BIGINT,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- PROCESS_LOG_DETAIL\n","        CREATE TABLE IF NOT EXISTS {meta_db}.PROCESS_LOG_DETAIL (\n","          process_log_detail_id BIGINT,\n","          process_log_id BIGINT,\n","          process_name STRING,\n","          log_detail_message STRING,\n","          sql_query STRING,\n","          start_time TIMESTAMP,\n","          elapsed_duration BIGINT,\n","          total_row_cnt BIGINT,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- SOURCE\n","        CREATE TABLE IF NOT EXISTS {meta_db}.SOURCE (\n","          source_id BIGINT,\n","          provider_id BIGINT,\n","          source_code STRING,\n","          source_type STRING,\n","          category STRING,\n","          sub_category STRING,\n","          source_system_name STRING,\n","          feed_cd STRING,\n","          stage_name STRING,\n","          count_threshold BIGINT,\n","          filelist_filename STRING,\n","          cr_flag STRING,\n","          cr_rekey_flag STRING,\n","          receipt_frequency STRING,\n","          receipt_day BIGINT,\n","          receipt_time STRING,\n","          post_validation_sql STRING,\n","          purge_type STRING,\n","          purge_max_length BIGINT,\n","          purge_max_length_type STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- SOURCE_FEED\n","        CREATE TABLE IF NOT EXISTS {meta_db}.SOURCE_FEED (\n","          source_id BIGINT,\n","          validate_prior_to_load_ind STRING,\n","          compression_str STRING,\n","          date_format STRING,\n","          time_format STRING,\n","          timestamp_format STRING,\n","          escape_char STRING,\n","          escape_unenclosed_field STRING,\n","          trim_space STRING,\n","          null_if STRING,\n","          error_on_column_count_mismatch STRING,\n","          on_error STRING,\n","          purge_str STRING,\n","          return_failed_only STRING,\n","          enforce_length STRING,\n","          truncatecolumns STRING,\n","          force_str STRING,\n","          acceptinvchars STRING,\n","          target_object_name STRING,\n","          add_record_id_ind STRING,\n","          split_filename_pattern STRING,\n","          strip_outer_element STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- SOURCE_FEED_COLUMN\n","        CREATE TABLE IF NOT EXISTS {meta_db}.SOURCE_FEED_COLUMN (\n","          column_id BIGINT,\n","          source_id BIGINT,\n","          ordinal_position BIGINT,\n","          column_name STRING,\n","          data_type STRING,\n","          max_length BIGINT,\n","          scale BIGINT,\n","          fixed_width_start_position BIGINT,\n","          fixed_width_length BIGINT,\n","          column_property STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- SOURCE_VIEW_INCREMENTAL\n","        CREATE TABLE IF NOT EXISTS {meta_db}.SOURCE_VIEW_INCREMENTAL (\n","          source_id BIGINT,\n","          source_code STRING,\n","          input_sql STRING,\n","          column_name STRING,\n","          last_pull_date STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- SOURCE_RANK\n","        CREATE TABLE IF NOT EXISTS {meta_db}.SOURCE_RANK (\n","          source_rank BIGINT,\n","          source_id BIGINT,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- TARGET\n","        CREATE TABLE IF NOT EXISTS {meta_db}.TARGET (\n","          target_id BIGINT,\n","          target_name STRING,\n","          receiver_id BIGINT,\n","          target_type STRING,\n","          category STRING,\n","          sub_category STRING,\n","          delivery_frequency STRING,\n","          delivery_day BIGINT,\n","          delivery_time STRING,\n","          purge_type STRING,\n","          purge_column STRING,\n","          purge_max_length BIGINT,\n","          purge_max_length_type STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- TARGET_VIEW\n","        CREATE TABLE IF NOT EXISTS {meta_db}.TARGET_VIEW (\n","          target_id BIGINT,\n","          view_name STRING,\n","          source_object_name STRING,\n","          stage_table_name STRING,\n","          mdb_table_name STRING,\n","          full_refresh_ind STRING,\n","          cr_ind STRING,\n","          cr_rekey_ind STRING,\n","          archive_table_ind STRING,\n","          fact_or_dimension STRING,\n","          natural_key STRING,\n","          order_by_cols STRING,\n","          surrogate_key STRING,\n","          generate_surr_key_ind STRING,\n","          archive_ret_copies BIGINT,\n","          source_data_2_best STRING,\n","          partition_by_cols STRING,\n","          indiv_id_change_col STRING,\n","          add_id_change_col STRING,\n","          house_id_change_col STRING,\n","          site_id_change_col STRING,\n","          batch_name STRING,\n","          batch_table STRING,\n","          id_change_table STRING,\n","          retain_col_list STRING,\n","          drop_stg_tables STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    spark.sql(f\"\"\"\n","        -- TARGET_VIEW_COLUMN_OVERRIDE\n","        CREATE TABLE IF NOT EXISTS {meta_db}.TARGET_VIEW_COLUMN_OVERRIDE (\n","          view_name STRING,\n","          column_name STRING,\n","          target_id BIGINT,\n","          mdb_table_name STRING,\n","          transformation_sql STRING,\n","          created_date TIMESTAMP,\n","          created_by STRING,\n","          modified_date TIMESTAMP,\n","          modified_by STRING\n","        ) USING DELTA\n","    \"\"\")\n","\n","except Exception as e:\n","    print(f\"||    Error: {e}\")\n","\n","print('||-----')\n","print('||----------------SUCCESS----------------||')\n","print('||-----')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"6e9f8c8d-eb02-4b70-a601-54973ff66a9b","normalized_state":"finished","queued_time":"2025-10-23T21:03:02.299737Z","session_start_time":null,"execution_start_time":"2025-10-23T21:03:02.3016341Z","execution_finish_time":"2025-10-23T21:03:40.5382053Z","parent_msg_id":"8cc07aea-9e7e-4530-acca-ea6cec8293ab"},"text/plain":"StatementMeta(, 6e9f8c8d-eb02-4b70-a601-54973ff66a9b, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["||-------------create_meta_tables.ipynb--------------||\n||-----\n||-----\n||    Environment: dev\n||    Meta Schema: meta_db\n||-----\n|| 0. Create meta tables if not exists\n||-----\n||-----\n||----------------SUCCESS----------------||\n||-----\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"source_hidden":false,"outputs_hidden":false}},"id":"8361b6fb-af34-4904-ad6d-4d9b0bba558b"},{"cell_type":"markdown","source":["#### Re-initialize Meta - Drop all meta tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76626110-b86b-4db1-be80-e163573af2e5"},{"cell_type":"code","source":["#---------------------------------------------------------------------\n","# Re-initialize Meta - Drop all meta tables\n","#---------------------------------------------------------------------\n","\n","print('||-----')\n","\n","print(f\"||    Environment: {environment}\")\n","print(f\"||    Meta Schema: {meta_db}\")\n","print('||-----')\n","\n","try:\n","\n","    # Set current database context\n","    spark.catalog.setCurrentDatabase(meta_db)\n","\n","    # Use SQL to list only physical tables in the meta schema\n","    tables_raw = spark.sql(f\"SHOW TABLES IN {meta_db}\")\n","    meta_tables = [\n","        row[\"tableName\"]\n","        for row in tables_raw.collect()\n","        if not row[\"isTemporary\"] and not row[\"tableName\"].endswith(\"_new\")\n","    ]\n","\n","    # Drop each meta table\n","    for table_name in meta_tables:\n","        print(f\"Dropping table: {meta_db}.{table_name}\")\n","        spark.sql(f\"DROP TABLE IF EXISTS {meta_db}.{table_name}\")\n","\n","except Exception as e:\n","    print(f\"||    Error: {e}\")\n","\n","print('||-----')\n","print('||----------------SUCCESS----------------||')\n","print('||-----')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"6e9f8c8d-eb02-4b70-a601-54973ff66a9b","normalized_state":"finished","queued_time":"2025-10-23T20:58:59.4011652Z","session_start_time":null,"execution_start_time":"2025-10-23T20:58:59.4022551Z","execution_finish_time":"2025-10-23T20:59:29.4998875Z","parent_msg_id":"95d43866-98ca-43ab-9a83-c25c3393fcf1"},"text/plain":"StatementMeta(, 6e9f8c8d-eb02-4b70-a601-54973ff66a9b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["||-----\n||    Environment: dev\n||    Meta Schema: meta_db\n||-----\nDropping table: meta_db.batch\nDropping table: meta_db.data_file\nDropping table: meta_db.data_object\nDropping table: meta_db.extract_config\nDropping table: meta_db.file_status_lkp\nDropping table: meta_db.process\nDropping table: meta_db.process_log\nDropping table: meta_db.process_log_detail\nDropping table: meta_db.provider\nDropping table: meta_db.receiver\nDropping table: meta_db.source\nDropping table: meta_db.source_feed\nDropping table: meta_db.source_feed_column\nDropping table: meta_db.source_rank\nDropping table: meta_db.source_view_incremental\nDropping table: meta_db.target\nDropping table: meta_db.target_view\nDropping table: meta_db.target_view_column_override\n||-----\n||----------------SUCCESS----------------||\n||-----\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"15c6d0a2-36d4-41a8-9a13-34a5d6ba2b8b"},{"cell_type":"markdown","source":["#### Verify Meta - Display records from each Meta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37160463-4426-44b1-878d-1f1384f0d166"},{"cell_type":"code","source":["#------------------------------------------------------------------------------------------\n","# Verify Meta - Display records from each Meta table\n","#------------------------------------------------------------------------------------------\n","\n","print('||-----')\n","print(f\"||    Environment: {environment}\")\n","print(f\"||    Meta Schema: {meta_db}\")\n","print('||-----')\n","\n","try:\n","    # Use SQL to list only physical tables in the meta schema\n","    tables_raw = spark.sql(f\"SHOW TABLES IN {meta_db}\")\n","    meta_tables = [\n","        row[\"tableName\"]\n","        for row in tables_raw.collect()\n","        if not row[\"isTemporary\"] and not row[\"tableName\"].endswith(\"_new\")\n","    ]\n","\n","    # Display records from each meta table\n","    for table_name in meta_tables:\n","        print(f\"{table_name}:\")\n","    \n","        # Run the query\n","        display(spark.sql(f\"\"\"\n","            SELECT * \n","            FROM {meta_db}.{table_name} \n","        \"\"\"))\n","\n","except Exception as e:\n","    print(f\"||    Error listing tables: {e}\")\n","\n","print('||-----')\n","print('||----------------SUCCESS----------------||')\n","print('||-----')\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a8d8be58-0dbb-41ea-a883-474eaf52194a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"f3800979-1825-4bca-82fa-6b9a29b49fea","known_lakehouses":[{"id":"f3800979-1825-4bca-82fa-6b9a29b49fea"}],"default_lakehouse_name":"lk_cdsa_bronze","default_lakehouse_workspace_id":"52b72164-8597-4107-9eb5-78a8f32516dc"}}},"nbformat":4,"nbformat_minor":5}