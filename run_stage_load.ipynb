{"cells":[{"cell_type":"code","source":["#---------------------------------------------------------------------\n","# Name: run_stage_load.ipynb\n","#---------------------------------------------------------------------\n","# Purpose:  Stages registered files from the landing zone to \n","#           delta stage tables in the bronze layer using expected\n","#           schema and source attributes from the meta data.\n","#---------------------------------------------------------------------\n","# ver.  | date     | author         | change\n","#---------------------------------------------------------------------\n","# v1    | 10/28/25 | K. Hardis      | Initial Version.\n","#---------------------------------------------------------------------\n","\n","# Standard library\n","import sys\n","import fnmatch\n","import os\n","\n","# PySpark SQL functions\n","from pyspark.sql.functions import col, current_timestamp\n","\n","# PySpark types\n","from datetime import datetime\n","\n","sys.path.append(\"./builtin\")\n","\n","# External Modules\n","import shared_context as sc\n","\n","import importlib\n","\n","# Force reload in case modules were cached\n","importlib.reload(sc)\n","\n","# Log external module versions\n","from log_module_versions import log_module_versions\n","log_module_versions([\"shared_context\"])\n","\n","print('||-------------run_stage_load.ipynb--------------||')\n","print('||-----')\n","\n","# Create spark shared context\n","ctx = sc.SparkContextWrapper(spark)\n","\n","# Load all REGISTERED files\n","registered_files_df = ctx.spark.sql(\"\"\"\n","    SELECT * FROM lk_cdsa_bronze.meta_db.data_file\n","    WHERE file_status = 'REGISTERED'\n","\"\"\")\n","\n","staged_count = 0\n","for file_row in registered_files_df.collect():\n","    try:\n","        file_id = file_row.file_id\n","        filename = file_row.filename\n","        object_id = file_row.object_id\n","        source_id = file_row.object_id\n","        batch_id = file_row.batch_id\n","        landing_directory = file_row.file_path\n","\n","        full_path = f\"abfss://CDSA@onelake.dfs.fabric.microsoft.com/lk_cdsa_landing_zone.Lakehouse{landing_directory}/{filename}\"\n","        print(f\"\\n|| Processing file_id: {file_id}\")\n","        print(f\"|| Filename: {filename}\")\n","        print(f\"|| Full path: {full_path}\")\n","\n","        # Load source attributes\n","        source_attr_df = ctx.spark.sql(f\"\"\"\n","            SELECT *, sf.source_id as source_feed_id, t.target_id as target_object_id\n","            FROM lk_cdsa_bronze.meta_db.source s\n","            JOIN lk_cdsa_bronze.meta_db.data_object do ON s.source_id = do.object_id\n","            LEFT JOIN lk_cdsa_bronze.meta_db.source_feed sf ON s.source_id = sf.source_id\n","            LEFT JOIN lk_cdsa_bronze.meta_db.target t ON sf.target_object_name = t.target_name\n","            WHERE s.source_id = '{source_id}'\n","        \"\"\")\n","\n","        if source_attr_df.count() == 0:\n","            print(f\"|| Skipping file_id {file_id}: No source attributes found.\")\n","            continue\n","\n","        source_row = source_attr_df.first()\n","        column_delimiter = source_row.column_delimiter or \",\"\n","        stage_name = source_row.stage_name\n","\n","        # Generate stage table name\n","        stage_table_name_df = ctx.spark.sql(f\"\"\"\n","            SELECT UPPER(LOWER(c.stage_name) || '_' || RIGHT('0000000' || CAST(a.file_id AS STRING), 7)) AS table_name\n","            FROM lk_cdsa_bronze.meta_db.data_file a\n","            JOIN lk_cdsa_bronze.meta_db.source c ON a.object_id = c.source_id\n","            WHERE a.file_id = {file_id}\n","        \"\"\")\n","        stage_table_name = stage_table_name_df.first()[\"table_name\"]\n","        print(f\"|| Stage table name: {stage_table_name}\")\n","\n","        # Get source feed columns\n","        feed_columns_df = ctx.spark.sql(f\"\"\"\n","            SELECT column_name, data_type, max_length, scale\n","            FROM lk_cdsa_bronze.meta_db.source_feed_column\n","            WHERE source_id = '{source_id}'\n","            ORDER BY ordinal_position, column_id\n","        \"\"\")\n","        feed_columns = feed_columns_df.collect()\n","\n","        # Load file\n","        df = ctx.spark.read \\\n","            .option(\"header\", True) \\\n","            .option(\"delimiter\", column_delimiter) \\\n","            .csv(full_path)\n","\n","        print(f\"|| Records read from file: {df.count()}\")\n","\n","        # Normalize column names\n","        df = df.toDF(*[col_name.lower() for col_name in df.columns])\n","\n","        # Check for schema mismatch\n","        expected_columns = [row.column_name.lower() for row in feed_columns]\n","        df_columns = [col.lower() for col in df.columns]\n","\n","        if set(expected_columns) != set(df_columns):\n","            print(f\"|| Schema mismatch for file_id {file_id}.\")\n","            print(f\"|| Expected columns: {expected_columns}\")\n","            print(f\"|| Found columns: {df_columns}\")\n","            continue\n","\n","        # Cast columns to expected types\n","        for row in feed_columns:\n","            col_name = row.column_name.lower()\n","            expected_type = row.data_type.lower()\n","\n","            if expected_type == \"decimal\":\n","                df = df.withColumn(col_name, col(col_name).cast(f\"decimal({int(row.max_length)},{int(row.scale)})\"))\n","            elif expected_type in [\"char\", \"varchar\", \"string\"]:\n","                df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n","            elif expected_type in [\"bigint\", \"long\"]:\n","                df = df.withColumn(col_name, col(col_name).cast(\"long\"))\n","            elif expected_type in [\"int\", \"integer\"]:\n","                df = df.withColumn(col_name, col(col_name).cast(\"int\"))\n","            elif expected_type == \"double\":\n","                df = df.withColumn(col_name, col(col_name).cast(\"double\"))\n","            elif expected_type == \"float\":\n","                df = df.withColumn(col_name, col(col_name).cast(\"float\"))\n","            elif expected_type == \"boolean\":\n","                df = df.withColumn(col_name, col(col_name).cast(\"boolean\"))\n","            elif expected_type == \"date\":\n","                df = df.withColumn(col_name, col(col_name).cast(\"date\"))\n","            elif expected_type == \"timestamp\":\n","                df = df.withColumn(col_name, col(col_name).cast(\"timestamp\"))\n","            else:\n","                df = df.withColumn(col_name, col(col_name).cast(\"string\"))  # fallback\n","\n","        # Reorder columns\n","        df = df.select(*expected_columns)\n","\n","        # Create stage table\n","        column_defs = []\n","        for row in feed_columns:\n","            col_name = row.column_name.lower()\n","            data_type = row.data_type.lower()\n","\n","            if data_type in [\"char\", \"varchar\"]:\n","                col_def = f\"{col_name} string\"\n","            elif data_type == \"decimal\":\n","                col_def = f\"{col_name} decimal({int(row.max_length)},{int(row.scale)})\"\n","            else:\n","                col_def = f\"{col_name} {data_type}\"\n","\n","            column_defs.append(col_def)\n","\n","        create_table_sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS lk_cdsa_bronze.bronze_db.{stage_table_name} (\n","            {', '.join(column_defs)}\n","        ) USING DELTA\n","        \"\"\"\n","        ctx.spark.sql(create_table_sql)\n","\n","        # Write to stage table\n","        output_table = f\"lk_cdsa_bronze.bronze_db.{stage_table_name}\"\n","        df.write.format(\"delta\").mode(\"append\").saveAsTable(output_table)\n","\n","        row_count = df.count()\n","        print(f\"|| Records staged to {stage_table_name}: {row_count}\")\n","\n","        # Update metadata\n","        ctx.spark.sql(f\"\"\"\n","            UPDATE lk_cdsa_bronze.meta_db.data_file\n","            SET row_count = {row_count}, file_status = 'COMPLETED'\n","            WHERE file_id = {file_id}\n","        \"\"\")\n","\n","        print(f\"|| ✅ Staged file_id {file_id} successfully.\")\n","        staged_count += 1\n","\n","    except Exception as e:\n","        print(f\"|| ❌ Error processing file_id {file_row.file_id}: {str(e)}\")\n","\n","\n","# Final output\n","if staged_count == 0:\n","    print(\"|| No REGISTERED files were staged.\")\n","    print(\"||----------------SKIPPED----------------||\")\n","else:\n","    print(f\"|| Stage load completed successfully for {staged_count} file(s).\")\n","    print(\"||----------------SUCCESS----------------||\")\n","\n","print('||-----')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"32d2602c-390a-4230-ba86-24cdb8f1df11","normalized_state":"finished","queued_time":"2025-10-29T14:11:37.4946457Z","session_start_time":"2025-10-29T14:11:37.4958778Z","execution_start_time":"2025-10-29T14:11:49.9011624Z","execution_finish_time":"2025-10-29T14:12:41.8193994Z","parent_msg_id":"e528bb34-d623-48b6-956a-400a7be67745"},"text/plain":"StatementMeta(, 32d2602c-390a-4230-ba86-24cdb8f1df11, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Module 'shared_context' loaded with version: sc_1.0\n||-------------run_stage_load.ipynb--------------||\n||-----\n\n|| Processing file_id: 1\n|| Filename: cdtq_customer_20251008.csv\n|| Full path: abfss://CDSA@onelake.dfs.fabric.microsoft.com/lk_cdsa_landing_zone.Lakehouse/Files/raw_customer_data/2025/10/08/cdtq_customer_20251008.csv\n|| Stage table name: STG_CDTQ_CUSTOMER_0000001\n|| Records read from file: 500\n|| Records staged to STG_CDTQ_CUSTOMER_0000001: 500\n|| ✅ Staged file_id 1 successfully.\n|| Stage load completed successfully for 1 file(s).\n||----------------SUCCESS----------------||\n||-----\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"fa0b5ad0-42a0-44e8-997a-6c396a85ac99"},{"cell_type":"code","source":["%%sql\n","update lk_cdsa_bronze.meta_db.data_file set file_status = 'REGISTERED' where file_id = 1;\n","drop table lk_cdsa_bronze.bronze_db.stg_cdtq_customer_0000001;\n","delete from lk_cdsa_bronze.meta_db.data_file;\n","delete from lk_cdsa_bronze.meta_db.batch;\n","\n","-- select * from lk_cdsa_bronze.meta_db.data_file;\n","-- select * from lk_cdsa_bronze.bronze_db.stg_cdtq_customer_0000001;\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false,"editable":false,"run_control":{"frozen":true}},"id":"fdc7de9e-1e9a-4438-9832-3963066f3021"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"fa1cb815-7f95-4c21-a806-4fe69b3c7feb"},{"id":"f3800979-1825-4bca-82fa-6b9a29b49fea"}],"default_lakehouse":"f3800979-1825-4bca-82fa-6b9a29b49fea","default_lakehouse_name":"lk_cdsa_bronze","default_lakehouse_workspace_id":"52b72164-8597-4107-9eb5-78a8f32516dc"}}},"nbformat":4,"nbformat_minor":5}