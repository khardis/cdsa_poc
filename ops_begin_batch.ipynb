{"cells":[{"cell_type":"code","source":["#---------------------------------------------------------------------\n","# Name: ops_begin_batch.ipynb\n","#---------------------------------------------------------------------\n","# Purpose:  Manages the initialization of ETL batches in the batch meta table.\n","#---------------------------------------------------------------------\n","# ver.  | date     | author         | change\n","#---------------------------------------------------------------------\n","# v1    | 10/28/25 | K. Hardis      | Initial Version.\n","#---------------------------------------------------------------------\n","\n","# Standard library\n","import sys\n","\n","# PySpark SQL functions\n","from pyspark.sql.functions import current_timestamp\n","\n","# PySpark types\n","from datetime import datetime\n","\n","sys.path.append(\"./builtin\")\n","\n","# External Modules\n","import shared_context as sc\n","\n","import importlib\n","\n","# Force reload in case modules were cached\n","importlib.reload(sc)\n","\n","# Log external module versions\n","from log_module_versions import log_module_versions\n","log_module_versions([\"shared_context\"])\n","\n","# Create spark shared context\n","ctx = sc.SparkContextWrapper(spark)\n","\n","# Parameters\n","batch_name = \"daily_update\"\n","batch_description = \"daily_update\"\n","batch_type = \"ETL\"\n","batch_status = \"STARTED\"\n","status_message = \"ops_begin_batch.py\"\n","\n","# Validate required input\n","if not batch_name:\n","    raise ValueError(\"batch_name argument is required\")\n","\n","# Print input summary\n","print(\"{\")\n","print(f'\"begin-batch_name\": \"{batch_name}\",')\n","print(f'\"begin-batch_description\": \"{batch_description}\",')\n","print(f'\"begin-batch_type\": \"{batch_type}\",')\n","print(f'\"begin-batch_status\": \"{batch_status}\",')\n","print(f'\"begin-status_message\": \"{status_message}\",')\n","\n","# Step 0: Get current max batch_id\n","max_id_row = spark.sql(\"SELECT COALESCE(MAX(batch_id), 0) AS max_id FROM meta_db.BATCH\").first()\n","new_batch_id = max_id_row[\"max_id\"] + 1\n","\n","# Step 1: Prepare timestamp strings\n","now_ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","# Step 2: Insert using spark.sql\n","current_user = 'system'\n","ctx.spark.sql(f\"\"\"\n","    INSERT INTO meta_db.BATCH\n","    VALUES (\n","        {new_batch_id},\n","        '{batch_name}',\n","        '{batch_description}',\n","        '{batch_type}',\n","        NULL,\n","        '{batch_status}',\n","        '{status_message}',\n","        TIMESTAMP('{now_ts}'),\n","        NULL,\n","        TIMESTAMP('{now_ts}'),\n","        '{current_user}',\n","        NULL,\n","        NULL\n","    )\n","\"\"\")\n","\n","\n","# Step 3: Confirm insert\n","print(f'\"end-msg\": \"end ops_begin_batch.py for {batch_name}\"')\n","print(\"}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"6e5ff71b-fb21-4f52-bdb1-a1fe9d44013b","normalized_state":"finished","queued_time":"2025-10-29T14:07:17.2282361Z","session_start_time":null,"execution_start_time":"2025-10-29T14:07:17.22945Z","execution_finish_time":"2025-10-29T14:07:29.1605995Z","parent_msg_id":"84bdf403-2982-440e-aa9f-dc1a9d5ed579"},"text/plain":"StatementMeta(, 6e5ff71b-fb21-4f52-bdb1-a1fe9d44013b, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Module 'shared_context' loaded with version: sc_1.0\n{\n\"begin-batch_name\": \"daily_update\",\n\"begin-batch_description\": \"daily_update\",\n\"begin-batch_type\": \"ETL\",\n\"begin-batch_status\": \"STARTED\",\n\"begin-status_message\": \"ops_begin_batch.py\",\n\"end-msg\": \"end ops_begin_batch.py for daily_update\"\n}\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3fab807e-712c-4e69-bdfa-35920df1aab8"},{"cell_type":"code","source":["%%sql\n","select * from lk_cdsa_bronze.meta_db.batch;\n","\n","-- delete from lk_cdsa_bronze.meta_db.batch;\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false,"editable":false,"run_control":{"frozen":true}},"id":"8a052a8c-c322-4cac-901c-5d43e062d97c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"f3800979-1825-4bca-82fa-6b9a29b49fea"}],"default_lakehouse":"f3800979-1825-4bca-82fa-6b9a29b49fea","default_lakehouse_name":"lk_cdsa_bronze","default_lakehouse_workspace_id":"52b72164-8597-4107-9eb5-78a8f32516dc"}}},"nbformat":4,"nbformat_minor":5}